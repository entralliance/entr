# Start from a core stack version
FROM jupyter/all-spark-notebook

# -------------------------------------------------------------------------------
# SECTION: initial setup & package updates
USER root
RUN apt-get update && \
    apt-get -y upgrade && \
    apt-get install -y libsasl2-modules-gssapi-heimdal && \
    apt-get install -y libsasl2-dev

## -------------------------------------------------------------------------------
## SECTION: Install and configure Python packages (including DBT)
USER jovyan
# RUN python -m venv .venv && .venv/bin/activate
RUN conda install -c conda-forge sasl
RUN pip install pandas matplotlib xgboost numpy sklearn xgboost sklearn scipy dbt-spark[PyHive]==1.0.0 sasl
# TBD: Install and configure OpenOA - depends on how we want to do this based on workflow

## -------------------------------------------------------------------------------
## SECTION: Configure Thrift server - needed so that thrift server can create the metastore_db directory
USER root
RUN chmod g+w /usr/local/spark

# -------------------------------------------------------------------------------
# SECTION: Entry Point
USER root
COPY startup_wrapper_script.sh /usr/local/bin
WORKDIR /home/jovyan
RUN ["chmod", "+x", "/usr/local/bin/startup_wrapper_script.sh"]
CMD /usr/local/bin/startup_wrapper_script.sh
